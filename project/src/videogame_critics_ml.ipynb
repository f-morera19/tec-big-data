{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Proyecto\n",
    "\n",
    "Autor: Fabian Morera Gutierrez\n",
    "\n",
    "Profesor: Dr. Juan Manuel Esquivel Rodriguez\n",
    "\n",
    "Curso: Big Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Entrenamiento de dos modelos de clasificación binaria para una variable de crítica especializada para un set de VideoJuegos.\n",
    "\n",
    "El presente programa tiene como finalidad la creación de dos modelos de clasificaciónbinaria, de principio a fin, utilizando el framework Apache Spark. Para tal fin, se utilizará un set de datos con información de ventas y crítica de videojuegos a nivel mundial.\n",
    "\n",
    "Se asume que dicho conjunto de datos fue previamente depurado, limpiado y preparado para su futura utilización como set de entrenamiento y testeo de un modelo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dependencias\n",
    "Importamos dependencias requeridas a lo largo del programa aquí."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy y Matplotlib.\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Others.\n",
    "import findspark\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "# Pyspark main.\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Pyspark machine learning.\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidatorModel, CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, HashingTF, Tokenizer, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Seaborn.\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Dependencies loaded.\")"
   ]
  },
  {
   "source": [
    "# Inspección de datos\n",
    "Previo a entrenar el modelo es común que se realice algún tipo de descripción de los datos, para tener una idea del tipo de problema con el que nos enfrentamos. A continuación, algunas observaciones interesantes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/usr/lib/python3.7/site-packages/pyspark')\n",
    "\n",
    "# Cargar el conjunto de datos completo. Este paso no realiza ningún ajuste; simplemente lectura\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Basic JDBC pipeline\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.2.14.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.2.14.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Cargar el conjunto de datos desde la base de datos (Tabla vg_critic_sales).\n",
    "videogames_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://host.docker.internal:5433/postgres\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"testPassword\") \\\n",
    "    .option(\"dbtable\", \"vg_critic_sales\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Data loaded!\")\n",
    "videogames_df.show()"
   ]
  },
  {
   "source": [
    "## Información Descriptiva de los Datos.\n",
    "Se muestran histogramas y resúmenes descriptivos de algunas de las variables disponibles."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información descriptiva de alguno valores relevantes del dataframe.\n",
    "videogames_df.describe([\n",
    "    'metascore', \n",
    "    'metacritic_user_score', \n",
    "    'total_shipped']).show()"
   ]
  },
  {
   "source": [
    "## Histogramas.\n",
    "Creamos algunos histogramas de relevancia, para visualizar la distribución de datos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de histogramas.\n",
    "def print_hist(rdd_histogram_data):\n",
    "    heights = np.array(rdd_histogram_data[1])\n",
    "    full_bins = rdd_histogram_data[0]\n",
    "    mid_point_bins = full_bins[:-1]\n",
    "    widths = [abs(i - j) for i, j in zip(full_bins[:-1], full_bins[1:])]\n",
    "    plt.bar(mid_point_bins, heights, width=widths, color='b')\n",
    "    plt.show()\n",
    "\n",
    "# Obtener histograma, con los parámetros de rango determinados\n",
    "# y para la propiedad especificada.\n",
    "def create_histogram(h_start=0, h_stop=100, h_step=5, h_prop):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        h_start: Histogram start value.\n",
    "        h_stop: Histogram end value.\n",
    "        h_step: Iteration size.\n",
    "        h_prop: Name of property to create Histogram for.\n",
    "    \"\"\"\n",
    "    buckets = np.arange(\n",
    "        start=h_start, \n",
    "        stop=h_stop, \n",
    "        step=h_step).tolist()\n",
    "\n",
    "    rdd_histogram_data = videogames_df\\\n",
    "        .select(h_prop)\\\n",
    "        .rdd\\\n",
    "        .flatMap(lambda x: x)\\\n",
    "        .histogram(buckets)\n",
    "\n",
    "    print_hist(rdd_histogram_data)\n",
    "\n",
    "# Metascore Histogram.\n",
    "create_histogram(0,100,5,'metascore')\n",
    "\n",
    "# Metacritic user scores histogram.\n",
    "create_histogram(0,10,0.5,'metacritic_user_score')\n",
    "\n",
    "# Total Shipped in Millions Histogram.\n",
    "create_histogram(0,50,5,'total_shipped')\n",
    "\n",
    "# Critic Positive in base 10 Histogram.\n",
    "create_histogram(0,100,10,'c_pos')\n",
    "\n",
    "# Critic Neutral in base 10 Histogram.\n",
    "create_histogram(0,100,10,'c_neu')\n",
    "\n",
    "# Critic Negative in base 10 Histogram.\n",
    "create_histogram(0,100,10,'c_neg')"
   ]
  },
  {
   "source": [
    "## Preparación de datos (Vectorización de features)\n",
    "Para realizar operaciones más detalladas es necesario expresar las filas originales en vectores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videogames_df = videogames_df \\\n",
    "    .drop('formatted_name')\n",
    "    .withColumn('label', \n",
    "        F.when(F.col(\"metascore\") > 70, 1)\n",
    "        .otherwise(0))\n",
    "\n",
    "print(\"Videogames clean Dataset before vectorization.\\n\")\n",
    "videogames_df.show()\n",
    "videogames_df.printSchema()\n",
    "\n",
    "# Para realizar operaciones más detalladas es necesario expresar las filas originales en vectores\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'critic_score', \n",
    "        'user_score', \n",
    "        'total_shipped',\n",
    "        'year',\n",
    "        'formatted_name',\n",
    "        'c_pos',\n",
    "        'c_neu',\n",
    "        'c_neg',\n",
    "        'metascore',\n",
    "        'u_pos',\n",
    "        'u_neu',\n",
    "        'u_neg',\n",
    "        'metacritic_user_score',\n",
    "        'isPCPlatform',\n",
    "        'isX360Platform',\n",
    "        'isOtherPlatform',\n",
    "        'isERating',\n",
    "        'isTRating',\n",
    "        'isMRating',\n",
    "        'isOtherRating',\n",
    "        'isActionGenre',\n",
    "        'isActionAdventureGenre',\n",
    "        'isOtherGenre',\n",
    "        'isOnePlayerOnly',\n",
    "        'hasOnlineMultiplayer'],\n",
    "    outputCol='features')\n",
    "\n",
    "vector_df = assembler.transform(videogames_df)\n",
    "vector_df = vector_df.select(['features', 'videogames_df'])\n",
    "vector_df.show()\n",
    "vector_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con la representación de vectores podemos calcular correlaciones\n",
    "pearson_matrix = Correlation.corr(vector_df, 'features').collect()[0][0]\n",
    "\n",
    "sns.heatmap(pearson_matrix.toArray(), annot=True, fmt=\".2f\", cmap='viridis')"
   ]
  },
  {
   "source": [
    "## Estandarización.\n",
    "\n",
    "Como recordamos de los módulos anteriores es deseable que los datos se encuentren estandarizados o normalizados, para evitar que la magnitud de ciertos atributos dominen el proceso de entrenamiento. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler(inputCol='features', outputCol='scaled')\n",
    "scale_model = standard_scaler.fit(vector_df)\n",
    "\n",
    "scaled_df = scale_model.transform(vector_df)\n",
    "scaled_df = scaled_df.drop(\"features\")\n",
    "scaled_df = scaled_df.withColumnRenamed(\"scaled\", \"features\")\n",
    "scaled_df.show()\n",
    "scaled_df.printSchema()"
   ]
  },
  {
   "source": [
    "## Clasificación Binaria \n",
    "Entrenamos dos modelos con técnicas diferentes y evaluamos los resultados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = scaled_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "print(\"Training Data Count: \", trainingData.count())\n",
    "trainingData.show()\n",
    "print(\"Test Data Count:\", testData.count())\n",
    "testData.show()"
   ]
  }
 ]
}